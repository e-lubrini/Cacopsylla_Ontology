{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!cd data/ & wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\\n!cd data/ & wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\\n!cd data/ & unzip wikitext-103-raw-v1.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!cd data/ & wget http://www.gutenberg.org/cache/epub/16457/pg16457.txt\n",
    "!cd data/ & wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "!cd data/ & unzip wikitext-103-raw-v1.zip'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(files, alg='WPC'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "    tokenizer.train(files, trainer) # training the tokenzier\n",
    "    tokenizer.save(\"./tokenizer-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Using vocabulary from ['data/pg16457.txt']=======\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb#ch0000004?line=6'>7</a>\u001b[0m trained_tokenizer \u001b[39m=\u001b[39m train_tokenizer(files, alg)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb#ch0000004?line=7'>8</a>\u001b[0m input_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThis is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb#ch0000004?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m tokenize(input_string, trained_tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb#ch0000004?line=9'>10</a>\u001b[0m tokens_dict[alg] \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mtokens\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/ner_correction/correction_modules/latin_endings/wordpiece_training.ipynb#ch0000004?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m, alg, \u001b[39m\"\u001b[39m\u001b[39m----\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "small_file = ['data/pg16457.txt']\n",
    "large_files = [f\"./data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "for files in [small_file, large_files]:\n",
    "    print(f\"========Using vocabulary from {files}=======\")\n",
    "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
    "        trained_tokenizer = train_tokenizer(files, alg)\n",
    "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍\"\n",
    "        output = tokenize(input_string, trained_tokenizer)\n",
    "        tokens_dict[alg] = output.tokens\n",
    "        print(\"----\", alg, \"----\")\n",
    "        print(output.tokens, \"->\", len(output.tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0791b7be171ebad212c24ef65290d1f031fc8c5ca9521baf1e29a38a9507073b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
