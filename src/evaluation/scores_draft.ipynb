{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING SCORES\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer, f1_score, recall_score, precision_score\n",
    "\n",
    "sys.stdout.write('COMPUTING SCORES\\n')\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "## Subdirectories ##\n",
    "####################\n",
    "import os\n",
    "\n",
    "def get_child_dir_paths(dir_path):\n",
    "    child_names = os.listdir(dir_path)\n",
    "    child_paths = [os.path.join(dir_path,child_name) for child_name in child_names]\n",
    "    dir_paths = [child_path for child_path in child_paths if os.path.isdir(child_path)]\n",
    "    return dir_paths\n",
    "\n",
    "pred_dir_path = 'data/pred'\n",
    "\n",
    "comp_filepaths = dict()\n",
    "\n",
    "c = 0\n",
    "\n",
    "for dirpath in get_child_dir_paths(pred_dir_path):\n",
    "    comp_filepath = []\n",
    "    for filepath in os.listdir(dirpath):\n",
    "        filepath = os.path.join(pred_dir_path,str(c),filepath)\n",
    "        if 'comp' in filepath:\n",
    "            comp_filepath.append(filepath)\n",
    "\n",
    "        comp_filepaths[c] = comp_filepath\n",
    "    c += 1\n",
    "\n",
    "# e.g. of pred_filepaths:\n",
    "# {0: [('data/pred/0/extracted_psyllid_entities.txt',\n",
    "#    'data/pred/0/extracted_psyllid_relations.txt')],\n",
    "#  1: [('data/pred/1/extracted_psyllid_entities.txt',\n",
    "#    'data/pred/1/extracted_psyllid_relations.txt')]}\n",
    "\n",
    "\n",
    "save_dir = 'data/evaluation_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING SCORES\n",
      "['data/pred/0/comparison.json']\n",
      "PAIRS\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb#W0sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39melse\u001b[39;00m:    \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb#W0sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         ref_df[a]\u001b[39m.\u001b[39mappend(v)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m a,v \u001b[39min\u001b[39;00m pair[\u001b[39m'\u001b[39;49m\u001b[39mpred\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mif\u001b[39;00m a \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ref_df\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/elubrini/GitHub/psylve/src/evaluation/scores_draft.ipynb#W0sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         pred_df[a] \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer, f1_score, recall_score, precision_score\n",
    "\n",
    "sys.stdout.write('COMPUTING SCORES\\n')\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "## Subdirectories ##\n",
    "####################\n",
    "import os\n",
    "\n",
    "def get_child_dir_paths(dir_path):\n",
    "    child_names = os.listdir(dir_path)\n",
    "    child_paths = [os.path.join(dir_path,child_name) for child_name in child_names]\n",
    "    dir_paths = [child_path for child_path in child_paths if os.path.isdir(child_path)]\n",
    "    return dir_paths\n",
    "\n",
    "pred_dir_path = 'data/pred'\n",
    "\n",
    "comp_filepaths = dict()\n",
    "\n",
    "c = 0\n",
    "\n",
    "for dirpath in get_child_dir_paths(pred_dir_path):\n",
    "    comp_filepath = []\n",
    "    for filepath in os.listdir(dirpath):\n",
    "        filepath = os.path.join(pred_dir_path,str(c),filepath)\n",
    "        if 'comp' in filepath:\n",
    "            comp_filepath.append(filepath)\n",
    "\n",
    "        comp_filepaths[c] = comp_filepath\n",
    "    c += 1\n",
    "\n",
    "# e.g. of pred_filepaths:\n",
    "# {0: [('data/pred/0/extracted_psyllid_entities.txt',\n",
    "#    'data/pred/0/extracted_psyllid_relations.txt')],\n",
    "#  1: [('data/pred/1/extracted_psyllid_entities.txt',\n",
    "#    'data/pred/1/extracted_psyllid_relations.txt')]}\n",
    "\n",
    "\n",
    "save_dir = 'data/evaluation_results/'\n",
    "\n",
    "\n",
    "#######################\n",
    "##### nested Â dict ####\n",
    "##### to dataframe ####\n",
    "#######################\n",
    "\n",
    "for dir_n,comp_filepath in comp_filepaths.items():\n",
    "\n",
    "    #open each json comparison.json file\n",
    "    print (comp_filepath)\n",
    "    with open(comp_filepath[0], 'r') as f:\n",
    "            ent_evals = json.load(f)\n",
    "    len(ent_evals.items())\n",
    "    \n",
    "    ref_df = dict()\n",
    "    pred_df = dict()\n",
    "\n",
    "    for docname,entities in ent_evals.items():\n",
    "        if docname == 'global':\n",
    "            continue \n",
    "        for entname,pairs_n_scores in entities.items():\n",
    "            for pair in pairs_n_scores['pairs']:\n",
    "                print('PAIRS')\n",
    "                #print((pair.keys())) # = 'ref', 'pred', 'sim', 'cat'\n",
    "                \n",
    "                for a,v in pair['ref'].items(): # attribute and value of entity\n",
    "                    if a not in ref_df.keys():\n",
    "                        ref_df[a] = []\n",
    "                    else:    \n",
    "                        ref_df[a].append(v)\n",
    "\n",
    "                for a,v in pair['pred'].items():\n",
    "                    if a not in ref_df.keys():\n",
    "                        pred_df[a] = []\n",
    "                    else:\n",
    "                        pred_df[a].append(v)\n",
    "\n",
    "                ref_df['dir_n'].append(dir_n)\n",
    "                pred_df['dir_n'].append(dir_n)\n",
    "\n",
    "                ref_df['docname'].append(docname)\n",
    "                pred_df['docname'].append(docname)\n",
    "                \n",
    "                pred_df['entname'].append(entname)\n",
    "                ref_df['entname'].append(entname)\n",
    "\n",
    "                print('\\nREF',ref_df.head())\n",
    "                \n",
    "                sizes = []\n",
    "                for k in ref_df.keys():\n",
    "                    sizes.append(len(ref_df[k]))\n",
    "                for k in ref_df.keys():\n",
    "                    if len(ref_df[k]) < max(sizes):\n",
    "                        ref_df[k].extend([math.nan]*(max(sizes) - len(ref_df[k])))\n",
    "                sizes = []\n",
    "                for k in pred_df.keys():\n",
    "                    sizes.append(len(pred_df[k]))\n",
    "                for k in pred_df.keys():\n",
    "                    if len(pred_df[k]) < max(sizes):\n",
    "                        pred_df[k].extend([math.nan]*(max(sizes) - len(pred_df[k])))\n",
    "\n",
    "    ref_df = pd.DataFrame(ref_df)\n",
    "    pred_df = pd.DataFrame(pred_df)\n",
    "    print(pred_df.head())\n",
    "\n",
    "################\n",
    "## DUMMY DATA ##\n",
    "################\n",
    "\n",
    "## Create dummy data to be visualised using sklearn tools.\n",
    "\n",
    "dummies = {'False Positive': (0,1),\n",
    "            'False Negative': (1,0),\n",
    "            'True Positive': (1,1),\n",
    "            'True Negative': (0,0),\n",
    "            }\n",
    "\n",
    "dummies_inv = {v: k for k, v in dummies.items()}\n",
    "\n",
    "def create_dummy(data):\n",
    "    dummy_data = []\n",
    "    for docname,entities in data.items():\n",
    "        if (type(entities) is float) or (type(entities) is list):\n",
    "            continue\n",
    "        for entname,pairs_n_scores in entities.items():\n",
    "            if (type(pairs_n_scores) is float) or (type(pairs_n_scores) is list):\n",
    "                continue\n",
    "            for pair in pairs_n_scores['pairs']:\n",
    "                label = pair['cat']\n",
    "                dummy_data.append(dummies[label])\n",
    "\n",
    "    dummy_df = pd.DataFrame(dummy_data, columns =['ref', 'pred'])\n",
    "    \n",
    "    return dummy_df\n",
    "\n",
    "dummies_df = dict()\n",
    "for k,ent_eval in ent_evals.items():\n",
    "    dummies_df[k] = create_dummy(ent_eval)\n",
    "\n",
    "## dummies_df.keys() are numbers of folders\n",
    "print('DUMMIES DF KEYS')\n",
    "print(dummies_df.keys())\n",
    "\n",
    "for k,dummy_df in dummies_df.items():\n",
    "    ref_dummy, pred_dummy = dummy_df['ref'], dummy_df['pred']\n",
    "    dummies_df[k]=list(zip(ref_dummy, pred_dummy))\n",
    "\n",
    "\n",
    "# plotting confusion matrix\n",
    "conf_mat = confusion_matrix(ref_dummy, pred_dummy)\n",
    "sns.set(font_scale=1)\n",
    "matrix = sns.heatmap(conf_mat, annot=True, fmt='d', linewidths=.5, cmap='flare')\n",
    "matrix.set(xlabel='predicted', ylabel='actual')\n",
    "matrix.figure.savefig(\"data/evaluation_results/confusion_matrix.png\") \n",
    "\n",
    "print(classification_report(ref_dummy, pred_dummy))\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "## VISUALS ##\n",
    "#############\n",
    "\n",
    "from collections import Counter\n",
    "from tools.visual import plot\n",
    "\n",
    "counts =dict()\n",
    "\n",
    "for k,dummy_df in dummies_df.items():\n",
    "    counts[k] = dict(Counter(dummy_df))\n",
    "counts\n",
    "\n",
    "df = pd.DataFrame([count.values() for count in counts.values()],columns=[dummies_inv[k] for k in counts[0].keys()])\n",
    "#df = df.reset_index()\n",
    "df\n",
    "\n",
    "rows = []\n",
    "for col_name in df.columns:\n",
    "    col = df[col_name]\n",
    "    for i in df.index:\n",
    "        rows.append((i,col_name,df[col_name].iloc[i]))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=['i','cat','val'])\n",
    "\n",
    "\n",
    "x = 'i'\n",
    "hue = 'cat'\n",
    "palette = sns.color_palette('tab10',n_colors=2)\n",
    "\n",
    "plot(x=x, hue=hue, data=df,\n",
    "    title = 'Feature impact' ,\n",
    "    type='dist',\n",
    "    save_dir = save_dir,\n",
    "    )\n",
    "\n",
    "matrix.figure.savefig(\"data/evaluation_results/confusion_matrix.png\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0791b7be171ebad212c24ef65290d1f031fc8c5ca9521baf1e29a38a9507073b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
